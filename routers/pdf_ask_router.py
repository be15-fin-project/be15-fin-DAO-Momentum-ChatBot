from fastapi import APIRouter
from pydantic import BaseModel, Field
import json

from langchain_teddynote import logging
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores.faiss import FAISS
from langchain_community.document_loaders.pdf import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.chat_message_histories import ChatMessageHistory

from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain.chains.retrieval import create_retrieval_chain

from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import (
    PromptTemplate,
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    MessagesPlaceholder,
    HumanMessagePromptTemplate,
)
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.runnables.history import RunnableWithMessageHistory

from config import OPENAI_API_KEY, LANGCHAIN_PROJECT

# ========= LangSmith ì¶”ì  ì„¤ì • =========
logging.langsmith(project_name=LANGCHAIN_PROJECT)

# ========= ë¼ìš°í„° =========
router = APIRouter()

# ========= ë¬¸ì„œ ë²¡í„°í™” =========
loader = PyMuPDFLoader("data/ì‚¬ì´íŠ¸_ì„¤ëª…ì„œ_v2.pdf")
docs = loader.load()
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
split_docs = splitter.split_documents(docs)

embedding_model = OpenAIEmbeddings(model="text-embedding-3-large", openai_api_key=OPENAI_API_KEY)
vectorstore = FAISS.from_documents(split_docs, embedding_model)
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}
)

# ========= ì¶œë ¥ ëª¨ë¸ ë° íŒŒì„œ ì •ì˜ =========
class QAResponse(BaseModel):
    answer: str = Field(description="í•œêµ­ì–´ë¡œ ì‘ì„±ëœ ìì—°ìŠ¤ëŸ¬ìš´ ì„¤ëª…")
    endpoint: str = Field(description="ê´€ë ¨ëœ API endpoint ê²½ë¡œ. ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´")

qa_parser = JsonOutputParser(pydantic_object=QAResponse)

# ========= í”„ë¡¬í”„íŠ¸ ì •ì˜ =========
qa_prompt_template = PromptTemplate(
    input_variables=["context", "input", "roles"],
    template="""
You are a helpful assistant answering questions based on an internal system user guide.

Instructions:
1. If the question is clearly related to the internal guide (Context), answer based on the provided Context.
2. If the question is NOT related to the guide or is a general/common sense question, respond naturally as a chatbot would.
3. DO NOT mention or include the endpoint URL in the answer text itself.
4. If and only if there is a clearly related API endpoint, include it in your JSON under the "endpoint" key; otherwise set "endpoint" to an empty string.
5. Return only the raw JSON object. DO NOT include any markdown formatting (such as ```json or ```).
6. DO NOT include any explanatory text before or after the JSON.
7. The JSON keys must be in English.
8. All answers should be written in Korean (í•œêµ­ì–´).
9. If the user does not have an appropriate role to access the information, return the following message:
   {{
     "answer": "ì´ ì§ˆë¬¸ì€ í˜„ì¬ ê¶Œí•œìœ¼ë¡œ ì—´ëŒí•  ìˆ˜ ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.",
     "endpoint": ""
   }}
10. ë‹µë³€ì€ ìµœëŒ€ 300ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”. ë¬¸ì¥ì€ ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ ì •ë³´ ìœ„ì£¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”.

Your response should follow this JSON format:
{{
  "answer": "",
  "endpoint": ""
}}

User Roles:
{roles}

Context:
{context}

Question:
{input}
"""
)

qa_prompt = ChatPromptTemplate.from_messages([
    MessagesPlaceholder("history"),
    SystemMessagePromptTemplate(prompt=qa_prompt_template)
])

contextualize_q_system_prompt = """
Given a chat history, the latest user question, employee summary information (included in the first history message), \
and metadata about the HR management system tables, formulate a standalone question. \
The standalone question must be understandable without the chat history and \
should utilize the given employee summary and table metadata if relevant. 

**Do NOT answer the question. Only generate or reformulate the question.**

ì§ˆë¬¸ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ ì¡´ëŒ“ë§ë¡œ í•´ì¤˜.
"""

contextualize_q_prompt = ChatPromptTemplate.from_messages([
    ("system", contextualize_q_system_prompt),
    MessagesPlaceholder("history"),
    ("human", "{input}")
])

# ========= LLM ë° ì²´ì¸ êµ¬ì„± =========
llm = ChatOpenAI(model="gpt-4o", temperature=0.7, api_key=OPENAI_API_KEY)

contextual_retriever = create_history_aware_retriever(
    llm=llm,
    retriever=retriever,
    prompt=contextualize_q_prompt
)

# output parser ì—°ê²°ëœ ë¬¸ì„œ ì‘ë‹µ ì²´ì¸
combine_docs_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt) | qa_parser

rag_chain = create_retrieval_chain(
    retriever=contextual_retriever,
    combine_docs_chain=combine_docs_chain
)

# ========= ì„¸ì…˜ ê´€ë¦¬ =========
chat_history_store = {}

def get_chat_history(session_id: str) -> ChatMessageHistory:
    if session_id not in chat_history_store:
        chat_history_store[session_id] = ChatMessageHistory()
    return chat_history_store[session_id]

def truncate_history(history: ChatMessageHistory, max_turns=4):
    truncated = history.messages[-(max_turns * 2):]
    history.clear()
    for msg in truncated:
        history.add_message(msg)

chain_with_history = RunnableWithMessageHistory(
    runnable=rag_chain,
    get_session_history=get_chat_history,
    input_messages_key="input",
    history_messages_key="history"
)

# ========= ìš”ì²­ ëª¨ë¸ =========
class QueryRequest(BaseModel):
    query: str
    session_id: int
    roles: list[str]

# ========= ì—”ë“œí¬ì¸íŠ¸ =========
@router.post("/ask")
async def ask_conversational_question(query_request: QueryRequest):
    history = get_chat_history(query_request.session_id)
    truncate_history(history)

    response = chain_with_history.invoke(
        {
            "input": query_request.query,
            "roles": ", ".join(query_request.roles),
            "context": ""  # contextëŠ” retriever ë‚´ë¶€ì—ì„œ ìë™ ì£¼ì…
        },
        config={
            "tags": ["conversational-rag", LANGCHAIN_PROJECT],
            "configurable": {"session_id": query_request.session_id}
        }
    )

    # íˆìŠ¤í† ë¦¬ì— ì‚¬ìš©ì ì§ˆë¬¸ ì €ì¥
    history.add_user_message(HumanMessage(content=query_request.query))

    try:
        # ì‘ë‹µ íŒŒì‹±
        if isinstance(response, str):
            parsed = json.loads(response)
        elif isinstance(response, dict):
            parsed = response
        else:
            raise ValueError("Unknown response type")

        # â¬‡ï¸ answerê°€ dictì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        nested = parsed.get("answer")
        print("âœ… parsed:", parsed)
        print("âœ… nested (parsed['answer']):", nested)

        if isinstance(nested, dict):
            answer_text = nested.get("answer", "")
            endpoint_text = nested.get("endpoint", "")
        elif isinstance(nested, str):
            answer_text = nested
            endpoint_text = parsed.get("endpoint", "")
        else:
            answer_text = str(nested)
            endpoint_text = parsed.get("endpoint", "")

        print("âœ… answer_text:", answer_text)
        print("âœ… endpoint_text:", endpoint_text)

        history.add_ai_message(AIMessage(content=answer_text))

        # ğŸ” ì—…ë°ì´íŠ¸ëœ íˆìŠ¤í† ë¦¬ ì¶œë ¥
        print("=== Chat History AFTER invoke ===")
        for msg in history.messages:
            print(f"[{msg.type.upper()}] {msg.content}")
        print("=================================")

        return {
            "answer": answer_text,
            "endpoint": endpoint_text
        }

    except json.JSONDecodeError:
        history.add_ai_message(AIMessage(content="ì‘ë‹µ í˜•ì‹ì„ ì´í•´í•˜ì§€ ëª»í–ˆì–´ìš”. ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì‹œê² ì–´ìš”?"))
        return {
            "question": query_request.query,
            "answer": "ì‘ë‹µ í˜•ì‹ì„ ì´í•´í•˜ì§€ ëª»í–ˆì–´ìš”. ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì‹œê² ì–´ìš”?",
            "raw_output": response
        }

    except Exception as e:
        history.add_ai_message(AIMessage(content="ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”."))
        return {
            "question": query_request.query,
            "answer": "ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”.",
            "error": str(e),
            "raw_output": response
        }
